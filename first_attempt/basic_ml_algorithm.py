import numpy as np

# Generate random input data to train on (normally: from some external source)
observations = 1000
xs = np.random.uniform(low=-10, high=10, size=(observations, 1))
zs = np.random.uniform(-10, 10, (observations, 1))
inputs = np.column_stack((xs, zs))

# Generate target function f(x,z) = 2*x - 3*z + 5 + noise  data always creates noise, makes data slightly random
noise = np.random.uniform(-1, 1, (observations, 1))
targets = 2 * xs - 3 * zs + 5 + noise

# Gradient descent starts with number that should not be random, but we select a small initial weight.
# 0.1 is the radius of the range from which we initialise weights and biases â†’ between -0.1 and 0.1
init_range = 0.1

weights = np.random.uniform(-init_range, init_range, size=(2, 1))  # vector

biases = np.random.uniform(-init_range, init_range,
                           size=1)  # scalar. There are as many biases as there are outputs in machine learning

# Set Learning rate eta:
learning_rate = 0.02

# Train the model (need to find optimal number of iterations for model)
# Minimise loss function with respect to the weights and biases
for i in range(250):
    outputs = np.dot(inputs,
                     weights) + biases  # python adds scalars to matrices element-wise, which is exactly what we want
    deltas = outputs - targets
    loss = np.sum(deltas ** 2) / 2 / observations  # Using L2-norm loss/2

    print(loss)

    deltas_scaled = deltas / observations  # update weights and biases
    weights = weights - learning_rate * np.dot(inputs.T, deltas_scaled)  # .T transposes matrix
    biases = biases - learning_rate * np.sum(deltas_scaled)

# Targets: 2; -3; 5
print(weights, biases)

######################## Output ########################

# [[ 2.00112256]
#  [-3.00245002]] [4.87169942]

######################## Output Loss ########################
# 240.90143798630086
# 37.86452986868244
# 14.433049181248572
# 11.362359830686778
# 10.630947389494656
# 10.183039611175886
# 9.782208812693643
# 9.400639687877424
# 9.03456891681247
# 8.683035101826267
# 8.345422378077211
# 8.021174967086681
# 7.709763244063117
# 7.410679033362068
# 7.1234343322507385
# 6.847560462577311
# 6.582607299988377
# 6.328142538783948
# 6.083750986471599
# 5.849033886316041
# 5.6236082667143235
# 5.407106316327491
# 5.199174783948349
# 4.999474402125983
# 4.807679333606593
# 4.623476639687471
# 4.446565769616627
# 4.276658070204999
# 4.11347631485106
# 3.9567542512094325
# 3.80623616676539
# 3.661676471606513
# 3.522839297710667
# 3.389498114096515
# 3.261435357208646
# 3.1384420759342055
# 3.020317590671887
# 2.906869165896934
# 2.7979116956879717
# 2.693267401702516
# 2.5927655431083956
# 2.496242137997776
# 2.403539695829274
# 2.314506960461556
# 2.2289986633592043
# 2.146875286568119
# 2.0680028350737567
# 1.992252618170771
# 1.919501039487313
# 1.8496293953214076
# 1.782523680960367
# 1.7180744046672132
# 1.656176409030601
# 1.5967286993867773
# 1.5396342790335908
# 1.4847999909677079
# 1.4321363658867976
# 1.381557476208674
# 1.3329807958692277
# 1.286327065670375
# 1.2415201639583215
# 1.1984869824211482
# 1.1571573068030465
# 1.1174637023405924
# 1.0793414037341258
# 1.0427282094746975
# 1.0075643803541974
# 0.9737925419930178
# 0.941357591226276
# 0.9102066061957933
# 0.8802887600011743
# 0.8515552377690871
# 0.8239591570054129
# 0.7974554911003399
# 0.7720009958615669
# 0.7475541389557577
# 0.7240750321431131
# 0.7015253661944967
# 0.679868348384931
# 0.6590686424614539
# 0.6390923109874113
# 0.6199067599690957
# 0.6014806856743898
# 0.5837840235566377
# 0.5667878992004118
# 0.5504645812091328
# 0.5347874359576731
# 0.5197308841361212
# 0.5052703590137952
# 0.49138226635541604
# 0.478043945924027
# 0.46523363450786037
# 0.4529304304108156
# 0.4411142593486113
# 0.4297658416949674
# 0.41886666102437886
# 0.40839893390015125
# 0.39834558085839583
# 0.3886901985406594
# 0.3794170329296965
# 0.37051095364473885
# 0.36195742925430047
# 0.3537425035662484
# 0.3458527728564532
# 0.33827536399886404
# 0.3309979134613218
# 0.3240085471328383
# 0.31729586094943907
# 0.31084890228693923
# 0.30465715209030825
# 0.2987105077104594
# 0.29299926642045443
# 0.28751410958424173
# 0.28224608745208213
# 0.2771866045578635
# 0.27232740569447567
# 0.267660562444362
# 0.2631784602432708
# 0.2588737859560986
# 0.2547395159445557
# 0.25076890460718304
# 0.24695547337302254
# 0.24329300013098157
# 0.23977550907764653
# 0.2363972609669808
# 0.233152743745992
# 0.2300366635611007
# 0.22704393612052587
# 0.22416967839859755
# 0.2214092006684653
# 0.21875799885019312
# 0.2162117471617692
# 0.21376629106102232
# 0.2114176404669484
# 0.2091619632493698
# 0.20699557897631451
# 0.20491495290891132
# 0.20291669023400175
# 0.20099753052505878
# 0.19915434242237517
# 0.1973841185238408
# 0.1956839704779747
# 0.1940511242712008
# 0.1924829157016826
# 0.19097678603233137
# 0.18953027781589107
# 0.18814103088529316
# 0.1868067785027341
# 0.1855253436611989
# 0.18429463553238845
# 0.1831126460552598
# 0.18197744665961357
# 0.18088718511937874
# 0.17984008253046432
# 0.178834430408245
# 0.17786858789994528
# 0.17694097910737394
# 0.17605009051564025
# 0.17519446852365697
# 0.17437271707239974
# 0.17358349536705514
# 0.17282551568933888
# 0.17209754129641447
# 0.1713983844029865
# 0.17072690424327364
# 0.17008200520969996
# 0.1694626350652684
# 0.16886778322669863
# 0.16829647911552847
# 0.1677477905744883
# 0.1672208223465646
# 0.16671471461427165
# 0.16622864159674638
# 0.16576181020237976
# 0.16531345873478395
# 0.1648828556499843
# 0.1644692983628119
# 0.1640721121005419
# 0.16369064880191456
# 0.1633242860597396
# 0.16297242610535612
# 0.16263449483329556
# 0.16230994086455366
# 0.16199823464694385
# 0.16169886759106325
# 0.16141135124046274
# 0.16113521647466705
# 0.16087001274374235
# 0.16061530733316606
# 0.1603706846577963
# 0.1601357455837906
# 0.15991010677736675
# 0.15969340007934352
# 0.15948527190444117
# 0.15928538266436096
# 0.15909340621370216
# 0.15890902931781328
# 0.15873195114170915
# 0.15856188275921929
# 0.15839854668156622
